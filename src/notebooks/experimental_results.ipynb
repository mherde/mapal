{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# import required packages\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.statistic_functions import read_results, compute_aulc_ranks, extract_execution_times\n",
    "from src.utils.data_functions import investigate_data_set\n",
    "from src.utils.plot_functions import plot_aulc_ranks, plot_learning_curves, create_latex_table\n",
    "\n",
    "from matplotlib import rc\n",
    "#rc('text', usetex=True)\n",
    "#rc('font', family='serif')\n",
    "rc('axes', edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "Here, you can define the setup of the conducted experiments. In particular, you can change the `data_set_type` whose explanation is given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_set_type = 'real-world' # real-world annotators\n",
    "#data_set_type = 'simulated-o' # simulated annotators having uniform performance values\n",
    "#data_set_type = 'simulated-y' # simulated annotator having class-dependent performance values\n",
    "#data_set_type = 'simulated-x' # simulated annotator having class-dependent performance values\n",
    "\n",
    "\n",
    "# path of results\n",
    "abs_path = '../../results/{}/csvs'.format(data_set_type)\n",
    "\n",
    "# parameters of experiments\n",
    "budget = 0.4\n",
    "test_ratio = 0.4\n",
    "n_repeats = 100 # number of repeated experiments\n",
    "\n",
    "# parameters of mapal\n",
    "#beta_0_list = ['1.0', '0.1', '0.01', '0.001', '0.0001', '0.00001']\n",
    "beta_0_list = ['0.0001']\n",
    "M_prime = '2'\n",
    "alpha = '1'\n",
    "\n",
    "# plot setup\n",
    "fontsize = 13\n",
    "\n",
    "# file name to store results\n",
    "dir_ranking = '../../results/{}/ranking-statistics/{}'.format(data_set_type, data_set_type)\n",
    "dir_learning = '../../results/{}/learning-curves/{}'.format(data_set_type, data_set_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Set Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_sets = pd.read_csv('../../data/data-set-names-{}.csv'.format(data_set_type), index_col='filename')\n",
    "data_set_df = {'name': [], 'n-features': [], 'n-instances': [], 'n-instances-per-class': [], 'annotation-perfs': []}\n",
    "filename_list = []\n",
    "for d in data_sets.iterrows():\n",
    "    filename = d[0]\n",
    "    filename_list.append(filename)\n",
    "    name = d[1][0]\n",
    "    n_features, n_instances_per_class, annotation_perfs = investigate_data_set(data_set_name=filename)\n",
    "    data_set_df['name'].append(name)\n",
    "    data_set_df['n-features'].append(n_features)\n",
    "    data_set_df['n-instances'].append(np.sum(n_instances_per_class))\n",
    "    data_set_df['n-instances-per-class'].append(np.array2string(n_instances_per_class, precision=2, separator=','))\n",
    "    data_set_df['annotation-perfs'].append(np.array2string(annotation_perfs, precision=2, separator=','))\n",
    "data_set_df = pd.DataFrame(data_set_df, index=filename_list).sort_values(by=['n-instances'])\n",
    "data_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(data_set_df.to_latex().replace('[', '').replace(']', '').replace(',', ', '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Learning Curves and Ranking Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for beta_0 in beta_0_list:\n",
    "    # setup name of strategies and load results\n",
    "    print('beta_0: {}'.format(beta_0))\n",
    "    filename_list_beta = ['mapal-1-{}-{}-{}-entropy'.format(beta_0, M_prime, alpha), 'ceal', 'ie-thresh', \n",
    "                          'ie-adj-cost', 'proactive', 'alio', 'random']\n",
    "    strategy_df_beta = {\n",
    "                   'name': ['MaPAL', 'CEAL', 'IEThresh', 'IEAdjCost', 'Proactive', 'ALIO', 'Random'],\n",
    "                   'color': ['g', 'r', 'm', 'grey', 'k', 'b', 'y'],\n",
    "                   'line': ['-', '-', '-', '-', '-', '-', '-']\n",
    "                  }\n",
    "    strategy_df_beta = pd.DataFrame(strategy_df_beta, index=filename_list_beta)\n",
    "    results_dict_beta = read_results(abs_path=abs_path, data_set_names=data_set_df.index.values, \n",
    "                                     strategy_names=strategy_df_beta.index.values, budget=budget, test_ratio=test_ratio)\n",
    "    \n",
    "    \n",
    "    # compute execution times\n",
    "    execution_times_df = pd.DataFrame(extract_execution_times(dic=results_dict_beta))\n",
    "    print(execution_times_df.T.to_latex(float_format=\"%.4f\"))\n",
    "    \n",
    "    # compute ranking statistics\n",
    "    data_set_keys = list(results_dict_beta.keys())\n",
    "    strategy_keys = list(results_dict_beta[data_set_keys[0]].keys())\n",
    "    data_set_names = data_set_df.loc[data_set_keys]['name']\n",
    "    strategy_names = strategy_df_beta.loc[strategy_keys]['name']\n",
    "    aulc_vals, aulc_ranks, test_results = compute_aulc_ranks(dic=results_dict_beta, c=strategy_keys[0])\n",
    "    aulc_mean = np.mean(aulc_vals, axis=-1)\n",
    "    aulc_std = np.std(aulc_vals, axis=-1)\n",
    "    mean_ranks = np.mean(aulc_ranks, axis=1, keepdims=True)\n",
    "    \n",
    "    # print aulc mean table\n",
    "    print(create_latex_table(aulc_mean=aulc_mean, aulc_std=aulc_std, test_results=test_results, \n",
    "                             data_set_names=data_set_names, strategy_names=strategy_names))\n",
    "    \n",
    "    # create evaluation plots\n",
    "    filetype = 'pdf'\n",
    "    plot_learning_curves(results_dict=results_dict_beta, strategy_df=strategy_df_beta, save=True, fontsize=fontsize, \n",
    "                         filename='{}-{}'.format(dir_learning, beta_0), filetype=filetype)\n",
    "    plot_aulc_ranks(aulc_ranks=aulc_ranks, test_results=test_results, xlabels=data_set_names, \n",
    "                    ylabels=strategy_names, filename='{}-{}'.format(dir_ranking, beta_0), filetype=filetype)\n",
    "    plot_aulc_ranks(aulc_ranks=mean_ranks, xlabels=[''], test_results=test_results, ylabels=strategy_names, \n",
    "                    plot_type='mean', filename='{}-{}'.format(dir_ranking, beta_0), filetype=filetype)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
